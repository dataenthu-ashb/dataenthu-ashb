# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IzMIakjYCJXtp6ApX7sa7dXf4uXuFFd1
"""

import numpy as np
import pandas as pd
import tensorflow as tf
import keras_tuner as kt
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.preprocessing import MinMaxScaler
import os
import math
import random
from tensorflow.keras import backend as K

K.clear_session()
#note install pip keras_tuner!
####################################################################################################################


seedje = 2
print(seedje)

SEED = seedje
os.environ['PYTHONHASHSEED'] = str(SEED)
np.random.seed(SEED)
tf.random.set_seed(SEED)
random.seed(SEED)
file_path = "residuals midasunemp2000-2025-USA-Monthly-LSTM-AllVars+Targets.xlsx"
df = pd.read_excel(file_path)

df = df.iloc[:, 1:]

scalers_Y = {}
predictions_dict = {}

endogenous_vars = 'UNEMP'
endogenous_data = df[endogenous_vars].values
endogenous_data_copy = endogenous_data.copy()
exogenous_data = df.drop(columns=[endogenous_vars]).values

split_idx = int(len(df) * 0.8)
amount_to_predict = len(df) - split_idx

window_size = 10
test_predictions_final = []


def create_sequences(X, y, window_size):
    X_seq, y_seq = [], []
    for i in range(len(y) - window_size):
        x_combined = np.hstack((X[i:i + window_size], y[i:i + window_size]))
        X_seq.append(x_combined)
        y_seq.append(y[i + window_size])
    return np.array(X_seq), np.array(y_seq)


exogenous_data_scaled = MinMaxScaler(feature_range=(0, 1)).fit_transform(exogenous_data[:-amount_to_predict])
dependent_var_scaled = MinMaxScaler(feature_range=(0, 1)).fit_transform(endogenous_data[:-amount_to_predict].reshape(-1, 1))

X_seq, y_seq = create_sequences(exogenous_data_scaled, dependent_var_scaled, window_size)
X_seq_train = X_seq[:-1].reshape((X_seq.shape[0] - 1, window_size, X_seq.shape[2]))
y_train = y_seq[:-1]

def build_lstm_model(hp):
    model = Sequential()
    num_lstm_layers = hp.Choice('num_lstm_layers', [1, 2])

    model.add(LSTM(
        units=hp.Int('units', min_value=10, max_value=50, step=10),
        activation=hp.Choice('activation', ['relu']),
        return_sequences=(num_lstm_layers == 2),
        input_shape=(window_size, X_seq_train.shape[2])
    ))
    model.add(Dropout(hp.Float('dropout', 0.1, 0.3, step=0.05)))

    if num_lstm_layers == 2:
        model.add(LSTM(
            units=hp.Int('units_layer2', min_value=10, max_value=30, step=10),
            activation=hp.Choice('activation_layer2', ['relu', 'selu']),
            return_sequences=False
        ))
        model.add(Dropout(hp.Float('dropout_layer2', 0.1, 0.3, step=0.05)))

    model.add(Dense(
        units=hp.Int('dense_units', min_value=5, max_value=20, step=5),
        activation='relu'
    ))
    model.add(Dense(1, activation='linear'))

    optimizer = tf.keras.optimizers.Adam(
        learning_rate=hp.Choice('learning_rate', [0.001, 0.005, 0.01]),
        beta_1=hp.Choice('beta_1', [0.9, 0.95, 0.99]),
        beta_2=hp.Choice('beta_2', [0.999, 0.9999]),
        epsilon=hp.Choice('epsilon', [1e-8, 1e-7])
    )

    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])
    return model

tuner = kt.BayesianOptimization(
    build_lstm_model,
    objective='val_loss',
    max_trials=10,
    directory='lstm_tuning',
    project_name='tune_lstm_model'
)

early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)

tuner.search(X_seq_train, y_train, epochs=15, validation_split=0.1,
                 callbacks=[early_stopping], batch_size=4, verbose=1)

best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]



#####################################################################################################################

MSE_total = []
MSE_difference_with_benchmark = []
MAD_total = []
MSE_benchmark = 0.1111040621
for i in range(25,26):
  seedje = 2
  print(seedje)

  SEED = seedje
  os.environ['PYTHONHASHSEED'] = str(SEED)
  np.random.seed(SEED)
  tf.random.set_seed(SEED)
  random.seed(SEED)

  # Load dataset
  #file_path = "2000-2025-USA-Monthly-LSTM-AllVars+Targets.xlsx"
  #file_path = "ResidualsUNEMP2000-2025-USA-Monthly-LSTM-AllVars+Targets.xlsx"
  #file_path = "NEPDATA_LSTM.xlsx"
  #file_path = "Residuals-2000-2025-Quarterly-EU-LSTM-ALLVars+Targets.xlsx"
  #file_path = "ResidualsPCE 2000-2025-Quarterly-EU-LSTM-ALLVars+Targets.xlsx"
  #file_path = "ResidualsUnEMP2000-2025-EU-Monthly-LSTM-AllVars+Target (1).xlsx"
  #file_path = "ResidualsPCE2000-2025-USA-Monthly-LSTM-AllVars+Targets.xlsx"
  #file_path = "ResidualsUNEMP2000-2025-USA-Monthly-LSTM-AllVars+Targets.xlsx"
  #file_path = "2000-2025-USA-LSTM-Quarterly-AllVars+Target (1).xlsx"
  #file_path = "PCE 2000-2025-Quarterly-EU-LSTM-ALLVars+Targets.xlsx"
  #file_path = "GDP2000-2025-Quarterly-EU-LSTM-ALLVars+Targets.xlsx"

  #file_path = "ResidualsMIDAS 2000-2025-EU-Monthly-LSTM-AllVars+Target.xlsx"
  #file_path = "RESIDUALSmidasGDP2000-2025-USA-LSTM-Quarterly-AllVars+Target.xlsx"
  #file_path = "RESIDUALSPCEMIDAS2000-2025-USA-Monthly-LSTM-AllVars+Targets.xlsx"
  #file_path = "RESIDUALSARIMAXUNEMPUSA2000-2025-USA-Monthly-LSTM-AllVars+Targets.xlsx"
  #file_path = "RESIDUALSARIMAXGDPUSA2000-2025-USA-LSTM-Quarterly-AllVars+Target.xlsx"
  #file_path = "ARIMAXRESIDUALSGDPEU2000-2025-Quarterly-EU-LSTM-ALLVars+Targets.xlsx"
  #file_path = "RESIDUALSARIMAXPCEEU2000-2025-Quarterly-EU-LSTM-ALLVars+Targets.xlsx"
  #file_path = "UNEMP2000-2025-USA-Monthly-LSTM-AllVars+Targets.xlsx"
  #file_path = "residuals midasunemp2000-2025-USA-Monthly-LSTM-AllVars+Targets.xlsx"
  file_path = "residuals midasunemp2000-2025-USA-Monthly-LSTM-AllVars+Targets.xlsx"
  #file_path = "arimaxresiduals2000-2025-EU-Monthly-LSTM-Covid.xlsx"
  df = pd.read_excel(file_path)

  # Drop first row if it contains NaNs
  #df = df.iloc[1:].reset_index(drop=True)
  # Drop the first column (assumed to be time/date)

  #eventueel wel weer aan zetten
  df = df.iloc[:, 1:]
  scalers_Y = {} #weeet niet of dit nu nog ook op deze manier moet?
  predictions_dict = {}
  # # Identify exogenous and endogenous variables
  # missing_counts = df.isna().sum()
  # exogenous_vars = missing_counts[missing_counts == 0].index.tolist()
  # endogenous_vars = missing_counts[missing_counts > 0].index.tolist()

  # Extract exogenous and endogenous data
  endogenous_vars = 'UNEMP'

  # Endogene variabele (afhankelijke variabele)
  endogenous_data = df[endogenous_vars].values  # Zorgt voor een 2D-array
  endogenous_data_copy = endogenous_data.copy()

  # Exogene variabelen (alle kolommen behalve de endogene)
  exogenous_data = df.drop(columns=[endogenous_vars]).values  # Alle andere kolommen als exogene variabelen
  split_idx = int(len(df) * 0.8)
  amount_to_predict = len(df) - int(len(df) * 0.8)
  # Define window size
  window_size = 10
  test_predictions_final = []
  # Function to create sequences
  def create_sequences(X, y, window_size):
      X_seq, y_seq = [], []
      for i in range(len(y) - window_size):
          x_combined = np.hstack((X[i:i + window_size], y[i:i + window_size]))
          X_seq.append(x_combined)
          y_seq.append(y[i + window_size])
      return np.array(X_seq), np.array(y_seq)


  for iteration in range(0,amount_to_predict-1):
    exogenous_data_this_iteration = exogenous_data[:-amount_to_predict+iteration+1] #plus1 because you also want to scale on your X (the t+1 x) that you use for prediction
    scaler_X = MinMaxScaler(feature_range=(0, 1))
    exogenous_data_scaled = scaler_X.fit_transform(exogenous_data_this_iteration)

    y_this_iteration = endogenous_data[:-amount_to_predict+iteration+1] #without plus1 since you dont want to scale based on that t+1 y value (ik doe plus 1 er voor nu toch bij)

    dependent_var = y_this_iteration.reshape(-1, 1)
    var= endogenous_vars
    scaler_y = MinMaxScaler(feature_range=(0, 1))
    dependent_var_scaled = scaler_y.fit_transform(dependent_var)
    scalers_Y[var] = scaler_y

    X_seq, y_seq = create_sequences(exogenous_data_scaled, dependent_var_scaled, window_size)
    X_seq_train = X_seq[:-1]
    y_train = y_seq[:-1]
    X_test_seq = X_seq[-1:]
    X_seq_train = X_seq_train.reshape((X_seq_train.shape[0], window_size, X_seq_train.shape[2]))


    #def build_lstm_model():
    model = build_lstm_model(best_hps)
    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
    model.fit(X_seq_train, y_train, epochs=25, batch_size=4, validation_split=0.2,
              shuffle=False, callbacks=[early_stopping], verbose=1)

    last_window = X_test_seq
    next_pred = model.predict(last_window.reshape(1, window_size, last_window.shape[2]))[0]
    test_predictions = []
    test_predictions.append(next_pred)
    test_predictions_alomgescaled = scaler_y.inverse_transform(np.array(test_predictions).reshape(-1, 1))
    test_predictions_final.append(test_predictions_alomgescaled)

  # Resultaten opslaan
  test_predictions_final = np.array(test_predictions_final).reshape(-1, 1)
  predictions_dict[var] = test_predictions_final.flatten()
  test_indices = range(len(df) - len(test_predictions_final), len(df))
  for var in predictions_dict:
      df.loc[test_indices, var] = predictions_dict[var]

  output_file_path = "predictions_with_original_data_de goede.xlsx"
  df.to_excel(output_file_path, index=False)
  print(f"Predictions successfully saved in: {output_file_path}")
  #print(X_seq_train[-1])
  print(".....................")
  print(predictions_dict)

  MSE_this_iteration = math.sqrt(sum(((test_predictions_final.flatten()-endogenous_data_copy[-amount_to_predict+1:])**2))/len(test_predictions_final))
  MSE_total.append(MSE_this_iteration)
  MSE_difference = MSE_this_iteration - MSE_benchmark
  MSE_difference_with_benchmark.append(MSE_difference)
  mad_THIS_iteration = (sum(abs((test_predictions_final.flatten()-endogenous_data_copy[-amount_to_predict+1:])))/len(test_predictions_final))
  MAD_total.append(mad_THIS_iteration)
  print(MSE_this_iteration)
  print(MSE_difference)

print(MSE_total)

pip install keras_tuner