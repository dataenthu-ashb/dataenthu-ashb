# -*- coding: utf-8 -*-
"""6-3-vgmdegoededeextrapolation tuner.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bKel3ozU3jJf5LSOoWBwla1b-ga6YeIM
"""

import shutil

# Remove the tuning directory to force retuning
shutil.rmtree('lstm_tuning', ignore_errors=True)

import numpy as np
import pandas as pd
import tensorflow as tf
import keras_tuner as kt
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from sklearn.preprocessing import MinMaxScaler
import os

SEED = 1234
os.environ['PYTHONHASHSEED'] = str(SEED)
np.random.seed(SEED)
tf.random.set_seed(SEED)
tf.keras.utils.set_random_seed(SEED) #############################deze echt erbij als je bij hyperparameter tunen dezelfde resultaten wilt hebben

# Load dataset
#file_path = "quarterly data for europe_wihtout2024Q4 & weird values.xlsx"
#file_path = "Quarterly_adjusted.xlsx"
file_path = "Quarterly_ready_for_explanation.xlsx"

df = pd.read_excel(file_path)
df_voor_later = df.copy()

df = df.iloc[1:].reset_index(drop=True)  # Drop first row if it contains NaNs
df = df.iloc[:, 1:]  # Drop the first column (assumed to be time/date)

# Identify exogenous and endogenous variables
missing_counts = df.isna().sum()
exogenous_vars = missing_counts[missing_counts == 0].index.tolist()
endogenous_vars = missing_counts[missing_counts > 0].index.tolist()

# Extract exogenous and endogenous data
exogenous_data = df[exogenous_vars].values
endogenous_data = df[endogenous_vars].values

exogenous_data_reversed = exogenous_data[::-1]
endogenous_data_reversed = endogenous_data[::-1]

# Normalize exogenous variables
scaler_X = MinMaxScaler(feature_range=(0, 1))
exogenous_data_scaled = scaler_X.fit_transform(exogenous_data_reversed)

scalers_Y = {}
predictions_dict = {}

window_size = 10

def create_sequences(X, y, window_size):
    X_seq, y_seq = [], []
    for i in range(len(y) - window_size):
        x_combined = np.hstack((X[i:i + window_size], y[i:i + window_size]))
        X_seq.append(x_combined)
        y_seq.append(y[i + window_size])
    return np.array(X_seq), np.array(y_seq)

def build_lstm_model(hp):
    model = Sequential()

    # Number of LSTM layers (1 or 2)
    num_lstm_layers = hp.Choice('num_lstm_layers', [1, 2])
    # First LSTM layer
    model.add(LSTM(
        units=hp.Int('units', min_value=10, max_value=50, step=10),
        activation=hp.Choice('activation', ['relu']),#, 'selu']),
        return_sequences=(num_lstm_layers == 2),
        input_shape=(window_size, X_train.shape[2])
    ))

    model.add(Dropout(hp.Float('dropout', 0.1, 0.3, step=0.05)))

    # Second LSTM layer (optional)
    if num_lstm_layers == 2:
        model.add(LSTM(
            units=hp.Int('units_layer2', min_value=10, max_value=30, step=10),
            activation=hp.Choice('activation_layer2', ['relu', 'selu']),
            return_sequences=False
        ))
        model.add(Dropout(hp.Float('dropout_layer2', 0.1, 0.3, step=0.05)))

    # Dense layers
    model.add(Dense(
        units=hp.Int('dense_units', min_value=5, max_value=20, step=5),
        activation='relu'
    ))

    model.add(Dense(1, activation='linear'))

    # Optimizer tuning
    learning_rate = hp.Choice('learning_rate', [0.001, 0.005, 0.01])
    beta_1 = hp.Choice('beta_1', [0.9, 0.95, 0.99])
    beta_2 = hp.Choice('beta_2', [0.999, 0.9999])
    epsilon = hp.Choice('epsilon', [1e-8, 1e-7])

    optimizer = Adam(learning_rate=learning_rate, beta_1=beta_1, beta_2=beta_2, epsilon=epsilon)

    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])
    return model

# Train and forecast each endogenous variable separately
for var in endogenous_vars:
    print(f"Processing variable: {var}")

    dependent_var = df[var].values.reshape(-1, 1)
    missing_idx = np.where(pd.isna(dependent_var))[0]
    missing_idx2 = np.where(pd.isna(dependent_var))[0][-1] + 1

    dependent_var_reversed = dependent_var[::-1]
    scaler_y = MinMaxScaler(feature_range=(0, 1))
    known_y = dependent_var_reversed[:-missing_idx2]
    dependent_var_scaled = dependent_var_reversed.copy()
    dependent_var_scaled[:-missing_idx2] = scaler_y.fit_transform(known_y)
    scalers_Y[var] = scaler_y

    X_seq, y_seq = create_sequences(exogenous_data_scaled, dependent_var_scaled, window_size)
    X_seq = X_seq.reshape((X_seq.shape[0], window_size, X_seq.shape[2])) ####################

    split_idx = missing_idx2
    X_train, y_train = X_seq[:-split_idx], y_seq[:-split_idx]
    X_test = X_seq[-split_idx:]


    tuner = kt.BayesianOptimization(
        build_lstm_model,
        objective='val_loss',
        max_trials=10,
        directory='lstm_tuning',
        project_name=f'tune_{var}'
    )

    early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)
    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=1e-6)

    tuner.search(X_train, y_train, epochs=15, validation_split=0.1, callbacks=[early_stopping, reduce_lr], batch_size=4, verbose=1)

    best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]

    # PRINT BEST hps
    print(f"Best hyperparameters for {var}:")
    for key in best_hps.values.keys():
        print(f"{key}: {best_hps.get(key)}")
    print("-" * 50)

    model = build_lstm_model(best_hps) ############moet de validation pslit hieronder er wel bij want je hebt al weinig data
    model.fit(X_train, y_train, epochs=25, batch_size=4, validation_split=0.1, shuffle=False, callbacks=[early_stopping, reduce_lr], verbose=1)
    #model.fit(X_train, y_train, epochs=25, batch_size=4, shuffle=False, callbacks=[early_stopping, reduce_lr], verbose=1)

    future_steps = missing_idx2
    last_window = X_test[0].copy()
    predictions = []

    for t in range(future_steps):
        next_pred = model.predict(last_window.reshape(1, window_size, X_test.shape[2]))[0]
        predictions.append(next_pred)

        last_window = np.roll(last_window, -1, axis=0)
        last_window[-1, :-1] = exogenous_data_scaled[-missing_idx2 + t]
        last_window[-1, -1] = next_pred

    predictions = scaler_y.inverse_transform(np.array(predictions).reshape(-1, 1))
    predictions_dict[var] = predictions[::-1].flatten()
#############################################
print('now come the final predictions')
print(predictions_dict)
for var in predictions_dict:
    missing_idx = df[var].isna()
    df.loc[missing_idx, var] = predictions_dict[var].flatten()
    print(predictions_dict[var].flatten())
############################################

output_file_path = "predictions_with_advanced_tuning.xlsx"
df.to_excel(output_file_path, index=False)

print(f"Predictions successfully saved in: {output_file_path}")

pip install keras_tuner